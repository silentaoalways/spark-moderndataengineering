default {
  appName = "spark-redis-stream-aggs-app"
  spark {
    settings {
        "spark.master" = "local[*]"
        "spark.sql.session.timeZone" = "UTC"

        # Configuring the DataSource (DataStreamReader)
        "spark.redis.host" = "redis"
        "spark.redis.port" = "6379"
        "spark.app.source.format" = "redis" # source format
        "spark.app.source.options.stream.keys" = "com:coffeeco:coffee:v1:orders" # the source redis stream
        "spark.app.source.options.stream.read.batch.size" = "100" #take up to 100 records per batch
        "spark.app.source.options.stream.read.block" = "1000" #wait up to 1 second while fetching new data

        # Configuring the Windowing / Watermark Operations
        "spark.app.groupBy.window.timestamp.column" = "timestamp"
        "spark.app.groupBy.window.duration" = "5 minutes"
        "spark.app.groupBy.window.slide.duration" = "5 minutes"
        "spark.app.groupBy.window.start.time" = "0 seconds"
        "spark.app.source.watermark.duration" = "5 minutes"

        # Configure the DataSink (DataStreamWriter)
        "spark.app.sink.format" = "parquet"
        "spark.app.sink.queryName" = "coffee_orders_aggs"
        "spark.app.sink.trigger.enabled" = "true"
        "spark.app.sink.trigger.type" = "process" # Once
        "spark.app.sink.processing.interval" = "30 seconds"
        "spark.app.sink.outputMode" = " append"
        "spark.app.sink.option.checkpointLocation" = "s3a://com.coffeeco.data/apps/spark-redis-stream-aggs-app/1.0.0/"
        "spark.app.sink.option.path" = "s3a://com.coffeeco.data/warehouse/silver/coffee_order_aggs"
        "spark.app.sink.output.tableName" = "silver.coffee_order_aggs"

        # Configure S3A Filesystem
        "spark.hadoop.fs.s3a.impl" = "org.apache.hadoop.fs.s3a.S3AFileSystem"
        "spark.hadoop.fs.s3a.endpoint" = "http://minio:9000"
        "spark.hadoop.fs.s3a.connection.ssl.enabled" = "false"
        "spark.hadoop.fs.s3a.access.key" = "minio"
        "spark.hadoop.fs.s3a.secret.key" = "minio_admin"
        "spark.hadoop.fs.s3a.path.style.access" = "true"
        "spark.hadoop.fs.s3a.block.size" = "512M"

        # File Handling
        "spark.hadoop.parquet.enable.summary-metadata" = "false"
        "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version" = "2"
        "spark.sql.parquet.compression.codec" = "snappy"
        "spark.sql.parquet.mergeSchema" = "false"
        "spark.sql.parquet.filterPushdown" = "true"

        # Hive Metastore, S3A Connection Declarations, and the Main Warehouse
        "spark.sql.warehouse.dir" = "s3a://com.coffeeco.data/warehouse"
        "spark.sql.catalogImplementation" = "hive"
        "spark.sql.hive.metastore.version" = "2.3.7"
        "spark.sql.hive.metastore.jars" = "builtin"
        "spark.sql.hive.metastore.sharedPrefixes" = "org.mariadb.jdbc,com.mysql.cj.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc"
        "spark.sql.hive.metastore.schema.verification" = "true"
        "spark.sql.hive.metastore.schema.verification.record.version" = "true"
    }
  }
}