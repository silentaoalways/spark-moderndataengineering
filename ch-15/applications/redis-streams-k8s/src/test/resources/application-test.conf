default {
  appName = "spark-stateful-coffee-aggs-test"
  spark {
    settings {
        "spark.sql.session.timeZone" = "UTC"
        "spark.redis.host" = "redis"
        "spark.redis.port" = "6379"
        "spark.app.source.format" = "redis" # source format
        "spark.app.source.options.stream.keys" = "com:coffeeco:coffee:v1:orders" # the source redis stream
        "spark.app.source.options.stream.read.batch.size" = "100" #take up to 100 records per batch
        "spark.app.source.options.stream.read.block" = "1000" #wait up to 1 second while fetching new data

        "spark.scheduler.mode" = "FAIR"
        "spark.scheduler.allocation.file" = "src/test/resources/fair-scheduler.xml"
        "spark.sql.shuffle.partitions" = "16"

        "spark.app.groupBy.window.timestamp.column" = "timestamp"
        "spark.app.groupBy.window.duration" = "5 minutes"
        "spark.app.groupBy.window.slide.duration" = "5 minutes"
        "spark.app.groupBy.window.start.time" = "0 seconds"
        "spark.app.source.watermark.duration" = "10 minutes"

        "spark.app.sink.format" = "memory"
        "spark.app.sink.queryName" = "order_aggs"
        "spark.app.sink.trigger.enabled" = "true"
        "spark.app.sink.trigger.type" = "process" # Once
        "spark.app.sink.processing.interval" = "1 seconds"
        "spark.app.sink.outputMode" = "append"

        "spark.sql.catalogImplementation" = "hive"
        "spark.sql.hive.metastore.version" = "2.3.7"
        "spark.sql.hive.metastore.jars" = "builtin"
        "spark.sql.hive.metastore.sharedPrefixes" = "org.mariadb.jdbc,com.mysql.cj.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc"
        "spark.sql.hive.metastore.schema.verification" = "true"
        "spark.sql.hive.metastore.schema.verification.record.version" = "true"
        "spark.sql.parquet.compression.codec" = "snappy"
        "spark.sql.parquet.mergeSchema" = "false"
        "spark.sql.parquet.filterPushdown" = "true"
        "spark.hadoop.parquet.enable.summary-metadata" = "false"
        "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version" = "2"
    }
  }

}